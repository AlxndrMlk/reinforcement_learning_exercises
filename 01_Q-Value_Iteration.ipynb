{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process\n",
    "\n",
    "**Markov Decision Process** (MDP) is a system consisting of three types of entities: states, actions and rewards. Going from state $s_1$ to another state $s'$, we need to pick an action that can lead us to $s'$. Each action has a transition probability $T$. \n",
    "\n",
    "For instance, we can find ourselves in a state $s_1$ with possible actions $a_0$ and $a_1$. Action $a_0$ might have transition probability of 1 that leads us back to  $s_1$ and transition probability of 0 that leads us to $s_2$. On the other hand, $a_1$ can have probability $0.1$ that we'll go back to $s_1$ and probability $0.9$ that we pass to $s_2$.\n",
    "\n",
    "Additionaly, transitions may return rewards. For instance, trandition from $s_1$ to itself via $a_0$ may return a reward of +50, while other transitions may return rewards of zero. In certain circumstances, in such a world it would be easy to stuck in $s_1$ and never explore the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Value Iteration Algorithm\n",
    "\n",
    "**Q-Value Iteration Algorithm** lets us find the optimal strategy for a given MDP and a given set of parameters.\n",
    "\n",
    "It's defined by the following formula:\n",
    "\n",
    "$$\\large Q_{k+1}(s, a) = \\sum_{s'} T(s, a, s')[R(s, a, s') + \\gamma \\cdot \\max_{a'} Q_k(s', a')]$$\n",
    "\n",
    "for all $(s', a)$\n",
    "\n",
    "where:\n",
    "\n",
    "* $T(s, a, s')$ is the transition probability between $s$ and $s'$ when choosing $a$\n",
    "* $R(s, a, s')$ is the reward returned by this transition\n",
    "* $\\gamma$ is the discount factor.\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's define an MDP with three states and a couple of possible actions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an MDP\n",
    "transition_probas = [ # shape => (s, a, s')\n",
    "    [[.7, .3, 0.], [1., 0., 0.], [.8, .2, 0.]],\n",
    "    [[0., 1., 0.], None, [0., 0., 1.]],\n",
    "    [None, [.8, .1, .1], None]\n",
    "]\n",
    "\n",
    "rewards = [ # shape => (s, a, s')\n",
    "    [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "    [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "    [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]\n",
    "]\n",
    "\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-Values\n",
    "Q_vals = np.full((3, 3), -np.inf)  # -> -inf stands for impossible actions\n",
    "\n",
    "# Iterate and fill in possible actions with zeros\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_vals[state, actions] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.],\n",
       "       [  0., -inf,   0.],\n",
       "       [-inf,   0., -inf]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "Q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the params\n",
    "gamma = .9\n",
    "N_ITER = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the algo\n",
    "\n",
    "def update_Q(gamma, n_iters):\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f'Iteration {i}')\n",
    "\n",
    "        Q_prev = Q_vals.copy()   # Always have a fresh copy of the previous step before updates\n",
    "        for s in range(len(possible_actions)):    # Iterate over n of states \n",
    "            for a in possible_actions[s]:\n",
    "                Q_vals[s, a] = np.sum([\n",
    "                    transition_probas[s][a][s_p] * (rewards[s][a][s_p] + gamma * np.max(Q_prev[s_p]))\n",
    "                    for s_p in range(len(possible_actions))\n",
    "                ])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 10\n",
      "Iteration 20\n",
      "Iteration 30\n",
      "Iteration 40\n"
     ]
    }
   ],
   "source": [
    "update_Q(.9, N_ITER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18.91891892, 17.02702702, 13.62162162],\n",
       "       [ 0.        ,        -inf, -4.87971488],\n",
       "       [       -inf, 50.13365013,        -inf]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try higher gamma\n",
    "Q_vals = np.full((3, 3), -np.inf)  # -> -inf stands for impossible actions\n",
    "\n",
    "# Iterate and fill in possible actions with zeros\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_vals[state, actions] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 10\n",
      "Iteration 20\n",
      "Iteration 30\n",
      "Iteration 40\n"
     ]
    }
   ],
   "source": [
    "update_Q(.95, N_ITER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21.73304188, 20.63807938, 16.70138772],\n",
       "       [ 0.95462106,        -inf,  1.01361207],\n",
       "       [       -inf, 53.70728682,        -inf]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
